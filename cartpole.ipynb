{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "# ENV_NAME = 'MountainCar-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.01\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "MODEL_PATH = f'{ENV_NAME}_model'\n",
    "FRAME_RATE = 0.05\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "FRAME_RATE = 0.05 if ENV_NAME in {'CartPole-v1'} else 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.should_learn = True\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        if os.path.exists(MODEL_PATH):\n",
    "            self.model = keras.models.load_model(MODEL_PATH)\n",
    "            print('Using predefined model')\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(24, input_dim=self.observation_space, activation='tanh'))\n",
    "            self.model.add(Dense(48, activation='tanh'))\n",
    "            self.model.add(Dense(self.action_space, activation='linear'))\n",
    "            self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE, decay=LEARNING_RATE_DECAY))\n",
    "            print('Creating new model')\n",
    "        self.iterations = 1\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, exploration_rate):\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def learn(self):\n",
    "        states_batch, q_values_batch = [], []\n",
    "        mini_batch = random.sample(\n",
    "            self.memory, min(len(self.memory), BATCH_SIZE))\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = reward if done else reward + GAMMA * np.max(self.model.predict(next_state)[0])\n",
    "            states_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        self.model.fit(np.array(states_batch), np.array(q_values_batch), batch_size=len(states_batch), verbose=0)\n",
    "        if self.exploration_rate > EXPLORATION_MIN:\n",
    "            self.exploration_rate *= EXPLORATION_DECAY\n",
    "            \n",
    "        if self.iterations % 10 == 0:\n",
    "            self.model.save(MODEL_PATH)\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return np.reshape(state, [1, self.observation_space])\n",
    "\n",
    "    def stats(self):\n",
    "        return {\n",
    "            \"exploration_rate\": self.exploration_rate,\n",
    "        }\n",
    "    \n",
    "    def play(self, exploration):\n",
    "        state = self.process_state(self.env.reset())\n",
    "        done = False\n",
    "        score = 0\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = self.act(state, exploration)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = self.process_state(next_state)\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        self.learn()\n",
    "        return actions, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predefined model\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves, score = agent.play(EXPLORATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_moves(moves):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for action in moves:\n",
    "        time.sleep(0.05)\n",
    "        env.step(action)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_moves(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "EXPLORATION = EXPLORATION_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(iterations, render=False):\n",
    "    global EXPLORATION\n",
    "    iterator = tqdm(range(iterations))\n",
    "    for i in iterator:\n",
    "        moves, score = agent.play(EXPLORATION)\n",
    "        EXPLORATION *= EXPLORATION_DECAY\n",
    "        scores.append(score)\n",
    "        if render:\n",
    "            iterator.set_description(f'Score: {score} [AVG: {np.mean(scores)}]')\n",
    "            render_moves(moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score: 15.0 [AVG: 25.2]: 100%|██████████| 10/10 [01:06<00:00,  6.66s/it]             \n"
     ]
    }
   ],
   "source": [
    "learn(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score: 98 [AVG: 31.25]: 100%|██████████| 50/50 [06:10<00:00,  7.41s/it]             \n"
     ]
    }
   ],
   "source": [
    "learn(50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moves, score = agent.play(0)\n",
    "render_moves(moves)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
