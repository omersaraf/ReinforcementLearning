{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.19.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "# ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "MODEL_PATH = f'{ENV_NAME}_model'\n",
    "PLAY_MODE = True\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.01\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "MODEL_PATH = f'{ENV_NAME}_model'\n",
    "FRAME_RATE = 0.05\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "FRAME_RATE = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, env):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.should_learn = True\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        self.action_space = env.action_space.n\n",
    "\n",
    "        if os.path.exists(MODEL_PATH):\n",
    "            self.model = keras.models.load_model(MODEL_PATH)\n",
    "            print('Using predefined model')\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Dense(24, input_dim=self.observation_space, activation='tanh'))\n",
    "            self.model.add(Dense(48, activation='tanh'))\n",
    "            self.model.add(Dense(self.action_space, activation='linear'))\n",
    "            self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE, decay=LEARNING_RATE_DECAY))\n",
    "            print('Creating new model')\n",
    "        self.iterations = 1\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, exploration_rate):\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return np.reshape(state, [1, self.observation_space])\n",
    "    \n",
    "    def learn(self):\n",
    "        states_batch, q_values_batch = [], []\n",
    "        mini_batch = random.sample(\n",
    "            self.memory, min(len(self.memory), BATCH_SIZE))\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = reward if done else reward + GAMMA * np.max(self.model.predict(next_state)[0])\n",
    "            states_batch.append(state[0])\n",
    "            q_values_batch.append(q_values[0])\n",
    "\n",
    "        self.model.fit(np.array(states_batch), np.array(q_values_batch), batch_size=len(states_batch), verbose=0)\n",
    "\n",
    "        if self.iterations % 10 == 0:\n",
    "            print(f'Saving model to {MODEL_PATH}')\n",
    "            self.model.save(MODEL_PATH)\n",
    "\n",
    "        self.iterations += 1\n",
    "        \n",
    "    def play(self, exploration, render=True):\n",
    "        state = self.process_state(self.env.reset())\n",
    "        if render:\n",
    "            env.render()\n",
    "        done = False\n",
    "        score = 0\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = self.act(state, exploration)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            if render:\n",
    "                time.sleep(FRAME_RATE)\n",
    "                env.render()\n",
    "            next_state = self.process_state(next_state)\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        if not PLAY_MODE:\n",
    "            self.learn()\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predefined model\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-151.73804950987062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partialy Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-209.11163479215594"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-124.62020103034948"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
